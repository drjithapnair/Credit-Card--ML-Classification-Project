# ğŸªª Credit Card Default Risk Prediction

A complete **Machine Learning pipeline** to predict the likelihood of a credit card customer defaulting on their next monthâ€™s payment.  
The aim is to deliver an **accurate**, **interpretable**, and **deployable** solution for **binary classification** â€” **Default (Yes)** or **No Default (No)**.  

---

## ğŸ“‚ Dataset  
ğŸ“Œ **Source:** [UCI Machine Learning Repository â€“ Default of Credit Card Clients](https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients)  
ğŸ“Œ **Samples:** `30,000` customers  
ğŸ“Œ **Features Include:**  
- ğŸ‘¤ **Demographics:** Age, Gender, Education, Marital Status  
- ğŸ’° **Credit Usage:** Credit limit, Bill amounts (last 6 months), Payments  
- ğŸ“… **Repayment Status:** Delay codes for past 6 months  
- ğŸ¯ **Target:** `Default Payment Next Month` â†’ (1 = Yes, 0 = No)  

---

## ğŸ¯ Objective  
> Build a predictive model to **classify** whether a client is likely to default in the upcoming month, enabling **financial institutions** to take **proactive risk control measures**.  

---

## ğŸ“Š Exploratory Data Analysis (EDA)  
ğŸ” **Key Insights:**  
- ğŸ“ˆ **Right-skewed** distribution in bill and payment amounts  
- ğŸ”— Correlation between repayment delays & default likelihood  
- ğŸ’³ Higher credit limit customers tend to default less frequently  

ğŸ“Œ **Visualizations Used:**  
- Count Plots, Pie Charts, Histograms  
- Correlation Heatmaps  
- Distribution & Box Plots for Outlier Detection  

---

## ğŸ§¹ Data Preprocessing  
âœ… Column renaming for clarity (`PAY_0` â†’ `SEP_PAY`)  
âœ… One-hot encoding for categorical variables  
âœ… Outlier handling with **IQR method**  
âœ… Feature scaling with **StandardScaler**  
âœ… Target encoding: Yes â†’ `1`, No â†’ `0`  

---

## ğŸ¤– Model Performance  

| ğŸ§  Model              | ğŸ¯ Accuracy | ğŸ¯ Precision | ğŸ”„ Recall | ğŸ“Š F1 Score |
|----------------------|------------|-------------|----------|------------|
| Logistic Regression  | 0.68       | 0.69        | 0.68     | 0.68       |
| Decision Tree        | 0.82       | 0.81        | 0.83     | 0.82       |
| **Random Forest** âœ… | **0.88**   | **0.93**    | **0.83** | **0.88**   |
| AdaBoost             | 0.86       | 0.93        | 0.78     | 0.85       |
| Gradient Boosting    | 0.87       | 0.94        | 0.80     | 0.86       |

ğŸ† **Best Model:** Random Forest after **GridSearchCV Hyperparameter Tuning**  

---

## ğŸš€ Deployment  
- ğŸ“¦ Final pipeline saved using **Joblib**  
- ğŸ”„ Reloadable for **real-time predictions** on new customer data  

---

## ğŸ›  Tech Stack  

**ğŸ“Œ Languages & Libraries:**  
- ğŸ Python  
- ğŸ”¢ NumPy, ğŸ“Š Pandas  
- ğŸ“ˆ Matplotlib, ğŸ–Œï¸ Seaborn  
- âš–ï¸ imbalanced-learn (**SMOTE**)  
- ğŸ¤– Scikit-learn  
- ğŸ’¾ Joblib  

**ğŸ“Œ Machine Learning Workflow:**  
- Classification Models: Logistic Regression, Decision Tree, Random Forest, AdaBoost, Gradient Boosting  
- Data Scaling: StandardScaler  
- Resampling: SMOTE  
- Model Evaluation: Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix  

**ğŸ“Œ Tools & Environment:**  
- ğŸ“ Jupyter Notebook  
- ğŸ’» VS Code  
- ğŸ”— Git & GitHub  

---

## ğŸ“Œ Conclusion  
This project shows how **machine learning** can be applied in **financial risk prediction**.  
It can be scaled for **banking systems**, **risk dashboards**, and **credit scoring applications**.  

---

## ğŸ“¬ Contact  
ğŸ”— **LinkedIn:**(https://www.linkedin.com/in/drjithapnair/)  
